00000000: addi x31, x0, 0               | x31 00000000 -> 00000000 ; x31=0x00000000 (0x00000000 + +0)
00000004: addi x1, x0, 291              | x1 00000000 -> 00000123 ; x1=0x00000123 (0x00000000 + +291)
00000008: addi x2, x0, 5                | x2 00000000 -> 00000005 ; x2=0x00000005 (0x00000000 + +5)
0000000c: slli x3, x1, 5                | x3 00000000 -> 00002460 ; x3=0x00002460 (0x00000123 << 5)
00000010: sll  x4, x1, x2               | x4 00000000 -> 00002460 ; x4=0x00002460 (0x00000123 << (0x00000005&31=5))
00000014: xor  x22, x3, x4              | x22 00000000 -> 00000000 ; x22=0x00000000 (0x00002460 ^ 0x00002460)
00000018: sltu x20, x0, x22             | x20 00000000 -> 00000000 ; x20=0x00000000 (0x00000000 <u 0x00000000)
0000001c: slli x21, x20, 0              | x21 00000000 -> 00000000 ; x21=0x00000000 (0x00000000 << 0)
00000020: or   x31, x31, x21            | x31 00000000 -> 00000000 ; x31=0x00000000 (0x00000000 | 0x00000000)
00000024: addi x0, x0, 0  # ASSERT SLLI==SLL sh=5  | (no arch write) ; x0 stays 0
00000028: addi x2, x0, 7                | x2 00000005 -> 00000007 ; x2=0x00000007 (0x00000000 + +7)
0000002c: srli x5, x1, 7                | x5 00000000 -> 00000002 ; x5=0x00000002 (0x00000123 >> 7)
00000030: srl  x6, x1, x2               | x6 00000000 -> 00000002 ; x6=0x00000002 (0x00000123 >> (0x00000007&31=7))
00000034: xor  x22, x5, x6              | x22 00000000 -> 00000000 ; x22=0x00000000 (0x00000002 ^ 0x00000002)
00000038: sltu x20, x0, x22             | x20 00000000 -> 00000000 ; x20=0x00000000 (0x00000000 <u 0x00000000)
0000003c: slli x21, x20, 1              | x21 00000000 -> 00000000 ; x21=0x00000000 (0x00000000 << 1)
00000040: or   x31, x31, x21            | x31 00000000 -> 00000000 ; x31=0x00000000 (0x00000000 | 0x00000000)
00000044: addi x0, x0, 0  # ASSERT SRLI==SRL sh=7  | (no arch write) ; x0 stays 0
00000048: addi x7, x0, -8               | x7 00000000 -> fffffff8 ; x7=0xfffffff8 (0x00000000 + -8)
0000004c: addi x2, x0, 3                | x2 00000007 -> 00000003 ; x2=0x00000003 (0x00000000 + +3)
00000050: srai x8, x7, 3                | x8 00000000 -> ffffffff ; x8=0xffffffff (s32(0xfffffff8) >>> 3)
00000054: sra  x9, x7, x2               | x9 00000000 -> ffffffff ; x9=0xffffffff (s32(0xfffffff8) >>> (0x00000003&31=3))
00000058: xor  x22, x8, x9              | x22 00000000 -> 00000000 ; x22=0x00000000 (0xffffffff ^ 0xffffffff)
0000005c: sltu x20, x0, x22             | x20 00000000 -> 00000000 ; x20=0x00000000 (0x00000000 <u 0x00000000)
00000060: slli x21, x20, 2              | x21 00000000 -> 00000000 ; x21=0x00000000 (0x00000000 << 2)
00000064: or   x31, x31, x21            | x31 00000000 -> 00000000 ; x31=0x00000000 (0x00000000 | 0x00000000)
00000068: addi x0, x0, 0  # ASSERT SRAI==SRA sh=3 (neg)  | (no arch write) ; x0 stays 0
0000006c: addi x10, x0, 1               | x10 00000000 -> 00000001 ; x10=0x00000001 (0x00000000 + +1)
00000070: addi x11, x0, 32              | x11 00000000 -> 00000020 ; x11=0x00000020 (0x00000000 + +32)
00000074: sll  x12, x10, x11            | x12 00000000 -> 00000001 ; x12=0x00000001 (0x00000001 << (0x00000020&31=0))
00000078: addi x13, x0, 1               | x13 00000000 -> 00000001 ; x13=0x00000001 (0x00000000 + +1)
0000007c: xor  x22, x12, x13            | x22 00000000 -> 00000000 ; x22=0x00000000 (0x00000001 ^ 0x00000001)
00000080: sltu x20, x0, x22             | x20 00000000 -> 00000000 ; x20=0x00000000 (0x00000000 <u 0x00000000)
00000084: slli x21, x20, 3              | x21 00000000 -> 00000000 ; x21=0x00000000 (0x00000000 << 3)
00000088: or   x31, x31, x21            | x31 00000000 -> 00000000 ; x31=0x00000000 (0x00000000 | 0x00000000)
0000008c: addi x0, x0, 0  # ASSERT SLL mask rs2[4:0]  | (no arch write) ; x0 stays 0
00000090: addi x0, x0, 0                | (no arch write) ; x0 stays 0
00000094: addi x0, x0, 0                | (no arch write) ; x0 stays 0
00000098: addi x0, x0, 0                | (no arch write) ; x0 stays 0
0000009c: addi x0, x0, 0                | (no arch write) ; x0 stays 0
000000a0: addi x0, x0, 0                | (no arch write) ; x0 stays 0
000000a4: addi x0, x0, 0                | (no arch write) ; x0 stays 0
000000a8: addi x0, x0, 0                | (no arch write) ; x0 stays 0
000000ac: addi x0, x0, 0                | (no arch write) ; x0 stays 0
000000b0: addi x0, x0, 0                | (no arch write) ; x0 stays 0
000000b4: addi x0, x0, 0                | (no arch write) ; x0 stays 0
000000b8: addi x0, x0, 0                | (no arch write) ; x0 stays 0
000000bc: addi x0, x0, 0                | (no arch write) ; x0 stays 0
000000c0: addi x0, x0, 0                | (no arch write) ; x0 stays 0
000000c4: addi x0, x0, 0                | (no arch write) ; x0 stays 0
000000c8: addi x0, x0, 0                | (no arch write) ; x0 stays 0
000000cc: addi x0, x0, 0                | (no arch write) ; x0 stays 0
000000d0: addi x0, x0, 0                | (no arch write) ; x0 stays 0

FINAL REGFILE (x0..x31):
x00 = 0x00000000
x01 = 0x00000123
x02 = 0x00000003
x03 = 0x00002460
x04 = 0x00002460
x05 = 0x00000002
x06 = 0x00000002
x07 = 0xfffffff8
x08 = 0xffffffff
x09 = 0xffffffff
x10 = 0x00000001
x11 = 0x00000020
x12 = 0x00000001
x13 = 0x00000001
x14 = 0x00000000
x15 = 0x00000000
x16 = 0x00000000
x17 = 0x00000000
x18 = 0x00000000
x19 = 0x00000000
x20 = 0x00000000
x21 = 0x00000000
x22 = 0x00000000
x23 = 0x00000000
x24 = 0x00000000
x25 = 0x00000000
x26 = 0x00000000
x27 = 0x00000000
x28 = 0x00000000
x29 = 0x00000000
x30 = 0x00000000
x31 = 0x00000000
